{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49c5848f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# change tabula to tabula_middle_padding to test middle padding method\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "from tabula import Tabula \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dc690bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area_0', 'Wilderness_Area_1', 'Wilderness_Area_2', 'Wilderness_Area_3', 'Soil_Type_0', 'Soil_Type_1', 'Soil_Type_2', 'Soil_Type_3', 'Soil_Type_4', 'Soil_Type_5', 'Soil_Type_6', 'Soil_Type_7', 'Soil_Type_8', 'Soil_Type_9', 'Soil_Type_10', 'Soil_Type_11', 'Soil_Type_12', 'Soil_Type_13', 'Soil_Type_14', 'Soil_Type_15', 'Soil_Type_16', 'Soil_Type_17', 'Soil_Type_18', 'Soil_Type_19', 'Soil_Type_20', 'Soil_Type_21', 'Soil_Type_22', 'Soil_Type_23', 'Soil_Type_24', 'Soil_Type_25', 'Soil_Type_26', 'Soil_Type_27', 'Soil_Type_28', 'Soil_Type_29', 'Soil_Type_30', 'Soil_Type_31', 'Soil_Type_32', 'Soil_Type_33', 'Soil_Type_34', 'Soil_Type_35', 'Soil_Type_36', 'Soil_Type_37', 'Soil_Type_38', 'Soil_Type_39', 'Cover_Type']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义完整列名列表（共55列）\n",
    "covtype_columns = [\n",
    "    # 数值特征（10列）\n",
    "    'Elevation', 'Aspect', 'Slope', \n",
    "    'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n",
    "    'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', \n",
    "    'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points',\n",
    "    # Wilderness_Area 独热编码（4列）\n",
    "    'Wilderness_Area_0', 'Wilderness_Area_1', 'Wilderness_Area_2', 'Wilderness_Area_3',\n",
    "    # Soil_Type 独热编码（40列）\n",
    "    *[f'Soil_Type_{i}' for i in range(40)],\n",
    "    # 目标列\n",
    "    'Cover_Type'\n",
    "]\n",
    "\n",
    "# 读取数据（指定无表头，并用上面的列名赋值）\n",
    "original_data = pd.read_csv(\n",
    "    \"/drive1/zhd/Tabula/Real_Datasets/Covertype/covtype.data\",\n",
    "    header=None,  # 关键：告诉pandas数据无表头\n",
    "    names=covtype_columns  # 手动指定列名\n",
    ")\n",
    "\n",
    "# 现在获取的列名就是正确的了\n",
    "all_columns = original_data.columns.tolist()\n",
    "print(all_columns)  # 会包含你提到的45个分类列（4+40+1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e438a14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "修正后分类列数量： 45\n",
      "修正后分类列列表： ['Wilderness_Area_0', 'Wilderness_Area_1', 'Wilderness_Area_2', 'Wilderness_Area_3', 'Soil_Type_0', 'Soil_Type_1', 'Soil_Type_2', 'Soil_Type_3', 'Soil_Type_4', 'Soil_Type_5', 'Soil_Type_6', 'Soil_Type_7', 'Soil_Type_8', 'Soil_Type_9', 'Soil_Type_10', 'Soil_Type_11', 'Soil_Type_12', 'Soil_Type_13', 'Soil_Type_14', 'Soil_Type_15', 'Soil_Type_16', 'Soil_Type_17', 'Soil_Type_18', 'Soil_Type_19', 'Soil_Type_20', 'Soil_Type_21', 'Soil_Type_22', 'Soil_Type_23', 'Soil_Type_24', 'Soil_Type_25', 'Soil_Type_26', 'Soil_Type_27', 'Soil_Type_28', 'Soil_Type_29', 'Soil_Type_30', 'Soil_Type_31', 'Soil_Type_32', 'Soil_Type_33', 'Soil_Type_34', 'Soil_Type_35', 'Soil_Type_36', 'Soil_Type_37', 'Soil_Type_38', 'Soil_Type_39', 'Cover_Type']\n"
     ]
    }
   ],
   "source": [
    "# 步骤1：加载数据并定义完整分类列列表\n",
    "data = pd.read_csv(\"Real_Datasets/Covertype_compressed.csv\")\n",
    "\n",
    "# 定义Covertype数据集的分类列（4个Wilderness_Area + 40个Soil_Type + 1个Cover_Type，共45+1=46个）\n",
    "categorical_columns = [\n",
    "    \"Wilderness_Area_0\", \"Wilderness_Area_1\", \"Wilderness_Area_2\", \"Wilderness_Area_3\"\n",
    "] + [f\"Soil_Type_{i}\" for i in range(40)] + [\"Cover_Type\"]\n",
    "\n",
    "# 步骤2：强制转换分类列为object类型\n",
    "for col in categorical_columns:\n",
    "    data[col] = data[col].astype(\"object\")\n",
    "\n",
    "# 步骤3：验证分类列数量\n",
    "new_cat_cols = data.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "print(\"修正后分类列数量：\", len(new_cat_cols))  # 应输出46\n",
    "print(\"修正后分类列列表：\", new_cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5f700ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 初始化模型（大数据集训练30个epoch，参考文档策略）\n",
    "model = Tabula(\n",
    "    llm='distilgpt2', \n",
    "    experiment_dir=\"covertype_training\", \n",
    "    batch_size=32, \n",
    "    epochs=30, \n",
    "    categorical_columns=categorical_columns,\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43f166f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 初始化模型（大数据集训练30个epoch，参考文档策略）\n",
    "model = Tabula(\n",
    "    llm='distilgpt2', \n",
    "    experiment_dir=\"covertype_training\", \n",
    "    batch_size=32, \n",
    "    epochs=30, \n",
    "    categorical_columns=categorical_columns,\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837b11c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/drive1/zhd/Tabula/tabula/tabula.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `TabulaTrainer.__init__`. Use `processing_class` instead.\n",
      "  tabula_trainer = TabulaTrainer(self.model, training_args, train_dataset=tabula_ds, tokenizer=self.tokenizer,\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7278' max='544710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  7278/544710 1:17:22 < 95:14:54, 1.57 it/s, Epoch 0.40/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.593900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.515800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.510900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.508700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.507000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.505600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.504800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.503900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.502900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.502600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.501900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.501300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.501000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.500600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f52c8eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area_0', 'Wilderness_Area_1', 'Wilderness_Area_2', 'Wilderness_Area_3', 'Soil_Type_0', 'Soil_Type_1', 'Soil_Type_2', 'Soil_Type_3', 'Soil_Type_4', 'Soil_Type_5', 'Soil_Type_6', 'Soil_Type_7', 'Soil_Type_8', 'Soil_Type_9', 'Soil_Type_10', 'Soil_Type_11', 'Soil_Type_12', 'Soil_Type_13', 'Soil_Type_14', 'Soil_Type_15', 'Soil_Type_16', 'Soil_Type_17', 'Soil_Type_18', 'Soil_Type_19', 'Soil_Type_20', 'Soil_Type_21', 'Soil_Type_22', 'Soil_Type_23', 'Soil_Type_24', 'Soil_Type_25', 'Soil_Type_26', 'Soil_Type_27', 'Soil_Type_28', 'Soil_Type_29', 'Soil_Type_30', 'Soil_Type_31', 'Soil_Type_32', 'Soil_Type_33', 'Soil_Type_34', 'Soil_Type_35', 'Soil_Type_36', 'Soil_Type_37', 'Soil_Type_38', 'Soil_Type_39', 'Cover_Type']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7d3e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1. 保存训练后的模型（修正：epoch数=30，匹配Covertype大数据集训练策略；路径/文件名对齐）\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#torch.save(model.model.state_dict(), \"covertype_training/model_30epoch.pt\")\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m synthetic_data \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m synthetic_data\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovertype_30epoch.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/drive1/zhd/Tabula/tabula/tabula.py:201\u001b[0m, in \u001b[0;36mTabula.sample\u001b[0;34m(self, n_samples, start_col, start_col_dist, temperature, k, max_length, device)\u001b[0m\n\u001b[1;32m    199\u001b[0m already_generated \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n_samples \u001b[38;5;241m>\u001b[39m df_gen\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 201\u001b[0m     start_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtabula_start\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_start_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     start_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(start_tokens)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m# Generate tokens\u001b[39;00m\n",
      "File \u001b[0;32m/drive1/zhd/Tabula/tabula/tabula_start.py:153\u001b[0m, in \u001b[0;36mRandomStart.get_start_tokens\u001b[0;34m(self, n_samples)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_start_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_samples):\n\u001b[0;32m--> 153\u001b[0m     start_words \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     start_text \u001b[38;5;241m=\u001b[39m [s \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m start_words]\n\u001b[1;32m    155\u001b[0m     start_tokens \u001b[38;5;241m=\u001b[39m _pad_tokens(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(start_text)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/tabula/lib/python3.10/random.py:514\u001b[0m, in \u001b[0;36mRandom.choices\u001b[0;34m(self, population, weights, cum_weights, k)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a k sized list of population elements chosen with replacement.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03mIf the relative weights or cumulative weights are not specified,\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03mthe selections are made with equal probability.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \n\u001b[1;32m    512\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    513\u001b[0m random \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom\n\u001b[0;32m--> 514\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpopulation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cum_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 1. 保存训练后的模型（修正：epoch数=30，匹配Covertype大数据集训练策略；路径/文件名对齐）\n",
    "#torch.save(model.model.state_dict(), \"covertype_training/model_30epoch.pt\")\n",
    "\n",
    "\n",
    "synthetic_data = model.sample(n_samples=50000, max_length=200)\n",
    "synthetic_data.to_csv(\"covertype_30epoch.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b743035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def encode_data(df):\n",
    "    encoder = LabelEncoder()\n",
    "    for col in categorical_columns:\n",
    "        df[col] = encoder.fit_transform(df[col])\n",
    "    return df\n",
    "\n",
    "real_encoded = encode_data(data.copy())\n",
    "synth_encoded = encode_data(synthetic_data.copy())\n",
    "\n",
    "X_real, y_real = real_encoded.drop(\"Cover_Type\", axis=1), real_encoded[\"Cover_Type\"]\n",
    "X_synth, y_synth = synth_encoded.drop(\"Cover_Type\", axis=1), synth_encoded[\"Cover_Type\"]\n",
    "\n",
    "# 分割10%测试集（约5.8万行）\n",
    "X_real_train, X_real_test, y_real_train, y_real_test = train_test_split(\n",
    "    X_real, y_real, test_size=0.1, random_state=42, stratify=y_real\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_synth, y_synth)\n",
    "y_pred = rf.predict(X_real_test)\n",
    "synth_f1 = f1_score(y_real_test, y_pred, average=\"weighted\")\n",
    "print(f\"合成数据多分类F1-score：{synth_f1:.4f}（文档参考值：0.75±0.02）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11f2b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3249f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabula",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
